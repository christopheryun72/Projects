{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbWv0EOhxJpj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import requests\n",
    "import csv\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGSPOlvjxJpo"
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open('judgesSentencesOnly.csv', newline='') as file:\n",
    "    reader = csv.reader(file, delimiter=',')\n",
    "    for row in reader:\n",
    "        sentences.append(row)\n",
    "sents = [sent[0] for sent in sentences if len(sent[0]) > 2]\n",
    "print(sents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scrW5T6sxJpq"
   },
   "outputs": [],
   "source": [
    "sents = \" \".join(sents)\n",
    "sents[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVTVuRUJxJpt"
   },
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "  words = data.split()\n",
    "  table = str.maketrans('', '', string.punctuation)\n",
    "  words = [word.translate(table) for word in words]\n",
    "  words = [word for word in words if word.isalpha() ]\n",
    "  words = [word.lower() for word in words]\n",
    "  return words\n",
    "\n",
    "cleanSents = clean_text(sents)\n",
    "cleanSents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NemG6ZfSxJpv"
   },
   "outputs": [],
   "source": [
    "predLength = 15\n",
    "groups = []\n",
    "for i in range(predLength, len(cleanSents)):\n",
    "    part = cleanSents[i - predLength: i]\n",
    "    grouping = ' '.join(part)\n",
    "    groups.append(grouping)\n",
    "print(len(groups))\n",
    "groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mski0DrxJpy"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(groups)\n",
    "sequences = tokenizer.texts_to_sequences(groups)\n",
    "print(type(sequences))\n",
    "sequences = np.array(sequences)\n",
    "print(type(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tpq9zbJoxJp0"
   },
   "outputs": [],
   "source": [
    "print(np.shape(sequences))\n",
    "X = sequences[:, :-1]\n",
    "y = sequences[:, -1]\n",
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qo42ZGvIxJp2"
   },
   "outputs": [],
   "source": [
    "uniqueWords = 1 + len(tokenizer.word_index)\n",
    "uniqueWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7Sth0EfxJp6"
   },
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rBjpAN0xJp7"
   },
   "outputs": [],
   "source": [
    "rnnModel = tf.keras.models.Sequential()\n",
    "rnnModel.add(tf.keras.layers.Embedding(uniqueWords, 14, input_length=14))\n",
    "rnnModel.add(tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.2))\n",
    "rnnModel.add(tf.keras.layers.LSTM(256, return_sequences=False, dropout=0.3, recurrent_dropout=0.2))\n",
    "rnnModel.add(tf.keras.layers.Dense(units = 256, activation='relu'))\n",
    "rnnModel.add(tf.keras.layers.Dense(units = 256, activation='relu'))\n",
    "rnnModel.add(tf.keras.layers.Dense(units = uniqueWords, activation='softmax'))\n",
    "rnnModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWV3-2nlxJp9"
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1Guhq_lxJp_"
   },
   "outputs": [],
   "source": [
    "rnnModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics= ['accuracy'])\n",
    "rnnModel.fit(X, y, batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJ1R1l69xJqB"
   },
   "outputs": [],
   "source": [
    "def generate_text_seq(model, tokenizer, text_seq_length, seed_text, numWords):\n",
    "  predText = []\n",
    "  for _ in range(numWords):\n",
    "    encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    encoded = pad_sequences([encoded], maxlen=text_seq_length, truncating='pre')\n",
    "    y_pred = model.predict_classes(encoded)\n",
    "    predWord = ''\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "      if index == y_pred:\n",
    "        predWord = word\n",
    "        break\n",
    "    seed_text = seed_text + ' ' + predWord\n",
    "    predText.append(predWord)\n",
    "  return ' '.join(predText)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tw0QkasixJqC"
   },
   "outputs": [],
   "source": [
    "seed_text = groups[200]\n",
    "generate_text_seq(rnnModel, tokenizer, 14, seed_text, 15)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "RealRNNTextGeneration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
